# gesture-recognition

The goal of this project was to create an app which is able to recognize sign language gestures and translate them into text.

<p align="center">
  <img src="https://github.com/dirkzon/gesture-recognition/blob/master/example.gif" width="500">
</p>

## Contents
- [Features](#features)
- [Dataset](#dataset)
- [Citations](#citations)

## Features
- __multiple hands:__ multiple hands can be recognized at once
- __debug:__ visualize prediction skeleton and more

## Dataset
The [__*__](#citations) [Sign-Language-Digits-Dataset](https://github.com/ardamavi/Sign-Language-Digits-Dataset) was used to train and test the model. 
This dataset contains images for gestures '0' to '9'. Each gesture has around 200 images.


## Citations
- __*__ Mavi, A., (2020), “A New Dataset and Proposed Convolutional Neural 
Network Architecture for Classification of American Sign Language 
Digits”, arXiv:2011.08927 [cs.CV]
